{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOw38mEgt3qt7Tk/ygpdCeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superjuyeon/GWANGJU_AI_PROJECT/blob/master/1%EC%A3%BC%EC%B0%A8_%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlTgn-CExqR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx6opO-cyHlW",
        "colab_type": "text"
      },
      "source": [
        "#자율주행\n",
        "자율주행자동차 티볼리의 기능에는 여러가지가 있다. \n",
        "긴급제동 시스템(AEBS) 설정 - 60km/h 이하 주행 중 전방에 충돌이 예상될 경우 스스로 제동 브레이크를 작동하여 정지\n",
        "전방 추돌경보 시스템(FCWS) - 전방 차량과의 거리가 일정 수준 이하로 가까워지면 경고음을 울려 차량충돌의 위험을 감소\n",
        "스마트 하이빔(HBA) - 차량 감지 시 하향등으로 전환하여 마주오는 차량, 또는 선행차량의 눈부심을 방지 \n",
        "자율주행차는 인지단계를 통해 GPS와 외부 카메라, 레이더 등을 사용하여 주변 상황을 인식하고 정보를 수집한다. 내비게이션에 적용되어 있는 GPS는 오차가 10~30m 정도인데, 자율주행차 운행에 필요한 오차는 1m 이내이다. 판단 단계는 인지 단계에서 습득한 정보를 바탕으로 주행 전략을 결정하는 기술을 뜻한다. 자동차가 어떤 환경에 놓여있는지를 정확히 판단하고 외부 이미지를 분석하여 주행 환경에 맞는 경로를 설정한다. 인지 기술과 얼마나 잘 조화를 이루는지에 따라서 자율 주행의 완성도가 결정된다. 제어 단계는 본격적인 주행이 이뤄지는데, 엔진 구동과 주행 방향 등을 결정한다. 만약 인지 기술을 눈과 귀, 판단 단계를 두뇌라고 비유한다면, 제어는 직접 움직이는 팔과 다리이다. 이처럼 자율주행차는 인지와 판단 그리고 제어까지 세 개의 단계를 반복하며 소프트웨어가 자율주행차에 명령하고, 자율주행차는 명령에 따라 목적지까지 주행한다.\n",
        "\n",
        "#이미지 \n",
        "지금까지 검색이란 텍스트를 대상으로 한 것이었다. 우리는 흔히 동영상, 음악, 이미지 등의 콘텐츠를 검색하지만, 그것은 그 콘텐츠 자체가 아니라 콘텐츠에 달려있는 제목, 설명, 태그 등 텍스트를 검색한 것이다. 귀여운 고양이가 많이 등장하는 동영상이라고 할지라도 제목이나 부제, 내용설명, 태그 등에 고양이라는 단어가 포함돼 있지 않으면 ‘고양이’라는 검색어로 검색되지 않는다. 반대로 ‘고양이 닮은 강아지’라는 제목이 달린 동영상은 고양이가 한 마리 등장하지 않아도 ‘고양이’라는 검색어에 검색된다. 때문에 멀티미디어 콘텐츠 검색은 어떻게 제목이나 태그가 달려있느냐가 품질을 좌우했다. 실제 이미지와는 관계가 없다. 이것은 멀티미디어 검색의 본질적인 한계였다. 컴퓨터가 사물을 ‘보지’ 못하기 때문에 벌어지는 일이다. 세 살짜리 어린아이도 개와 고양이를 구분하는데, 컴퓨터는 하지 못한다. 구글은 이런 문제를 해결하기 위해 오랫동안 노력해왔고, 인공지능을 통해 해법을 찾았다. 딥러닝을 통해 컴퓨터가 이미지를 스스로 분류하거나 태깅을 할 수 있다. 기존에는 고양이 사진을 컴퓨터가 인지하기 위해 어떤 ‘평균적인 고양이 이미지 값’을 가지고 있었다. 고양이 사진인지 아닌지 컴퓨터가 알려면 이미지 픽셸 정보를 보고 그 값을 기존에 정의된 고양이 값과 비교해 판단한다. 그러나 고양이의 종류는 무수히 많으며, 웅크린 모양, 조명, 상황 등에 따라 그 값은 얼마든지 달라질 수 있다. 이 때문에 기존의 방식으로는 컴퓨터가 고양이를 인지하도록 하는 것은 사실상 불가능했다. 하지만 딥러닝은 이런 문제를 해결할 수 있다. 컴퓨터는 무수히 많은 고양이 사진을 입력받아 학습한다. 이 사이에서 공통점을 스스로 찾아내는 방식이다. 구글은 ‘오픈 이미지 데이터셋’이라는 컴퓨터 시각(Vision) 연구용 데이터셋을 이용한다. 900만개가 넘는 이미지로 구성된 이 데이터셋은 구글과 카네기맬론대, 코넬대가 공동으로 구축했다. 닐 알드린 구글 소프트웨어 엔지니어(컴퓨터 비전 리서치 담당)는 “이미 구글 인공지능에 적용된 컴퓨터 비전 모델의 능력은 사람보다 뛰어난 부분도 있다”며, “수년 내에 구글의 컴퓨터 비전이 인간보다 사물과 풍경을 더 잘 구분하는 시대가 올 것”이라고 말했다.\n",
        "\n",
        "#언어\n",
        "구글은 GNMT 번역 내용을 사람이 직접 감수한 결과 기존 PBMT 번역에 비해 오류가 평균 60%, 언어에 따라 58%(영어-중국어)에서 87%(영어-스페인어)까지 줄었다고 밝혔다. 이는 위키피디아나 뉴스 사이트에서 추출한 샘플 문장들을 번역한 결과다. 기존 PBMT 번역의 경우 입력되는 문장을 단어와 구절로 분해해 각기 이에 대응하는 외국어 단어와 구절로 옮긴 뒤 합성해 문장으로 출력하는 방식이다. 그러나 구글이 사용하는 GNMT 번역은 입력되는 문장을 통째로 읽어 번역한다. 기계가 방대한 단어와 구절, 문장을 기억하고 연관성이 없는 것은 하나씩 없애는 방식으로 정확한 해석을 해나갈 수 있는 학습능력과 방대한 데이터를 빠르게 처리할 수 있는 구조 덕분에 가능하다. 입력된 특정 단어에 대해 인공지능은 훈련에 사용된 방대한 '사전'(영어-프랑스어는 약 25억개 문장 쌍, 중국어는 5억개 문장 쌍)을 뒤져 적합한 단어를 찾아낸다. 개발팀은 GNMT 번역 기술에 대해 설명한 논문에서 입력-출력 전 과정이 하나의 신경망에서 이뤄진다고 설명했다.\n",
        "\n",
        "#음성\n",
        "2011년 10월 애플은 아이폰 4S 발표와 함께 '시리'라는 음성 인터페이스 기반 서비스가 포함되어 있음을 밝힌다. 시리는 초기에 영어, 독일어, 프랑스어를 지원했으며, 시간이 지나면서 다양한 언어로 확장을 한다. 시리는 SRI 인터네셔널이 미 국방연구소와 함께 군사 목적으로 추진한 인공지능 개발 프로젝트의 일환으로 개발된 기술을 상용화한 것이다. 이를 애플이 인수하여 아이폰에 탑재하여, 터치 인터페이스 기반 스마트폰에 음성 인터페이스를 추가하게 된 것이다. 시리 사용을 설정하면, 사용자가 'Hey siri' 또는 '시리야'와 같은 호출 명령을 하면 스마트폰 화면에 시리 음력 입력 화면이 나타나고 사용자의 음성을 인식하여 질문에 답을 하거나, 추천을 하거나, 특정 작업을 실행할 수 있도록 해준다. 동작원리는 간단하다. 사용자의 음성을 녹음하여 서버로 전달하면 뉘앙스사의 기술을 이용하여 음성을 텍스트로 변환한 후에 AI가 이를 분석하여 행동을 결정하여 결과를 전달하는 형식이다.\n",
        "초기에는 사용자의 음성 데이터가 많이 쌓이지 않아서 인식이 부정확하였지만 점차 데이터가 쌓이고 딥러닝을 통해 AI가 스스로 학습하여 보다 나은 결과를 만들어 주었다. 간단한 대화 부터, 농담, 노래부르기 등 일상적인 대화도 어느 정도 가능해서 초기에 사용자들은 '시리야, 사랑해' 같은 대화를 시도하기도 했다. 애플에서는 정책적으로 지속적이고 감정적인 대화를 하지 못하도록 제한시켜 놓았다고 한다. 스마트폰의 기본적인 기능인 전화걸기, 음악감상, 문자재생, 일정 관리, 웹 검색 및 시스템 설정 등을 시리를 통해서 진행을 할 수 있었으며, 앱과 통합되어 음성으로 문서를 작성하는 등의 작업 수행이 가능했다.\n",
        "\n"
      ]
    }
  ]
}